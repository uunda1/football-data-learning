# ğŸ“… Planning Anual â€” Football Data Learning

Este es el planning detallado de 52 semanas, empezando en **Febrero (Semana 1)**.  
Cada semana contiene **una Ãºnica tarea**, pensada para completarse en 3 horas.

---

# ğŸŸ¦ FEBRERO (Semanas 1â€“4)
## Semana 1
- [ ] Crear estructura mÃ­nima del repositorio

## Semana 2
- [ ] Repasar conceptos bÃ¡sicos de HTML y DOM

## Semana 3
- [ ] Crear archivo `legal-and-ethics.md` con consideraciones de scraping

## Semana 4
- [ ] Crear el primer scraper simple (Requests + BeautifulSoup)

---

# ğŸŸ¦ MARZO (Semanas 5â€“9)
## Semana 5
- [ ] Elegir 2 webs pÃºblicas para scraping

## Semana 6
- [ ] Extraer tablas estÃ¡ticas de las webs elegidas

## Semana 7
- [ ] Implementar paginaciÃ³n bÃ¡sica

## Semana 8
- [ ] Crear notebook `scraping-basics.ipynb`

## Semana 9
- [ ] Instalar y documentar Selenium (conceptos)

---

# ğŸŸ© ABRIL (Semanas 10â€“13)
## Semana 10
- [ ] Scraping dinÃ¡mico bÃ¡sico con Selenium

## Semana 11
- [ ] Crear scraper de partidos (dinÃ¡mico)

## Semana 12
- [ ] Crear notebook `scraping-matches.ipynb`

## Semana 13
- [ ] Refactorizar scrapers (cÃ³digo limpio en `src/`)

---

# ğŸŸ© MAYO (Semanas 14â€“17)
## Semana 14
- [ ] Repaso de API REST: mÃ©todos, tokens, headers

## Semana 15
- [ ] Probar API-Football u otra API gratuita

## Semana 16
- [ ] Guardar y analizar las primeras respuestas JSON

## Semana 17
- [ ] Documentar endpoints Ãºtiles para tu proyecto

---

# ğŸŸ© JUNIO (Semanas 18â€“21)
## Semana 18
- [ ] Crear `api_client.py` (cliente bÃ¡sico para APIs)

## Semana 19
- [ ] Implementar funciones: equipos, partidos, competiciones

## Semana 20
- [ ] Normalizar campos (nombres, fechas, identificadores)

## Semana 21
- [ ] Guardar datos procesados en `data/processed/`

---

# ğŸŸ§ JULIO (Semanas 22â€“25)
## Semana 22
- [ ] Crear pipeline automÃ¡tico (script principal)

## Semana 23
- [ ] Configurar cron / programador de tareas (segÃºn entorno)

## Semana 24
- [ ] Documentar pipeline endâ€‘toâ€‘end

## Semana 25
- [ ] Crear README completo para `02-api-extraction/`

---

# ğŸŸ§ AGOSTO (Semanas 26â€“29)
## Semana 26
- [ ] NormalizaciÃ³n avanzada de equipos (nombres, duplicados)

## Semana 27
- [ ] Limpieza de duplicados y manejo de datos faltantes

## Semana 28
- [ ] ValidaciÃ³n de fechas + formatos estÃ¡ndar

## Semana 29
- [ ] Crear funciones reutilizables para limpieza

---

# ğŸŸ§ SEPTIEMBRE (Semanas 30â€“33)
## Semana 30
- [ ] Modularizar scrapers (`src/`)

## Semana 31
- [ ] Modularizar APIs (`src/`)

## Semana 32
- [ ] AÃ±adir tests bÃ¡sicos (`tests/`)

## Semana 33
- [ ] Crear documentaciÃ³n de arquitectura de datos

---

# ğŸŸ¥ OCTUBRE (Semanas 34â€“37)
## Semana 34
- [ ] TeorÃ­a: proxies y cÃ³mo funciona el trÃ¡fico de apps

## Semana 35
- [ ] Analizar apps de fÃºtbol (SofaScore/FotMob) â€” solo teorÃ­a

## Semana 36
- [ ] Documentar endpoints observados (sin romper TOS)

## Semana 37
- [ ] Crear dataset controlado a partir de endpoints observados

---

# ğŸŸ¥ NOVIEMBRE (Semanas 38â€“41)
## Semana 38
- [ ] Definir tu proyecto final (tema, alcance)

## Semana 39
- [ ] Definir dataset final requerido

## Semana 40
- [ ] DiseÃ±ar arquitectura del proyecto final

## Semana 41
- [ ] Crear backlog del proyecto final

---

# ğŸŸ¥ DICIEMBRE (Semanas 42â€“45)
## Semana 42
- [ ] Implementar pipeline final

## Semana 43
- [ ] Generar dataset completo para el proyecto

## Semana 44
- [ ] Crear visualizaciones y anÃ¡lisis

## Semana 45
- [ ] Documentar el proyecto (README + notebooks)

---

# ğŸŸ¨ ENERO (Semanas 46â€“52)
## Semana 46
- [ ] Limpieza final del repositorio

## Semana 47
- [ ] Escribir README profesional del proyecto final

## Semana 48
- [ ] PublicaciÃ³n en GitHub

## Semana 49
- [ ] Preparar publicaciÃ³n en LinkedIn

## Semana 50
- [ ] Repaso general del aÃ±o

## Semana 51
- [ ] Mantenimiento del cÃ³digo y pipeline

## Semana 52
- [ ] Cierre del aÃ±o + reflexiÃ³n en `learning-journal.md`

---
